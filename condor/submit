#!/bin/bash

if ! voms-proxy-info -exists -hours 10 2> /dev/null; then
  echo 'create a proxy first'
  exit 1
fi

if (( $# < 3 )); then
  echo "usage: $0 batch_label jobs_per_input_file input_files..."
  exit 1
fi

# parse arguments
batchlabel=$1
jobsperinput=$2
inputfiles=${@:3}

# every batch needs a unique ID -- a timestamp works well
timestamp="$(date +%Y%m%d_%H%M%S)"

# gridftp destination for all events in this batch
desturl="gsiftp://ntheoryfs01.phy.duke.edu/var/phy/project/nukeserv/$USER/hic-events/$batchlabel"

# blacklist some hosts which systematically fail
badhosts=(
  'phys.uconn.edu'  # fails to load modules
  'lonepeak.peaks'  # fails to load modules (in a different way)
  'kingspeak.peaks' # same as lonepeaks
  'hpc.smu.edu'     # completes jobs but griftp transfer of results fails
)
# Create a regular expression to filter the condor "Machine" attribute.
# Match any item of the badhosts array at the end of the string:
#
#  (badhost1|badhost2|...)$
#
# Replace '.' with '\.' to match literal periods.
badhostsre() {
  local IFS='|'
  echo "(${badhosts[*]//\./\\.})$"
}

# change to project root directory, one level above this script
# http://stackoverflow.com/a/246128
cd "$(dirname "${BASH_SOURCE[0]}")"/..

# create a "scratch" directory for all the condor files
scratchdir="/local-scratch/$USER/condor/$batchlabel/$timestamp"
mkdir -p $scratchdir

# make package, place in scratch directory
pkgfile='hic-osg.tar.gz'
./makepkg $scratchdir

# copy condor executable to scratch
exefile='hic-wrapper'
cp -v condor/$exefile $scratchdir

# copy input files to scratch
inputfiledir="$scratchdir/inputfiles"
mkdir $inputfiledir
cp -v $inputfiles $inputfiledir

# Create a condor submit description file.  This is the "template" job file for
# every job in this batch with several $(...) variables to be set by the dag.
cat > $scratchdir/job <<EOF
universe = vanilla
x509userproxy = /tmp/x509up_u$(id --user)
+ProjectName = "Duke-QGP"

request_memory = 500M
request_disk = 500M
requirements = HAS_CVMFS_oasis_opensciencegrid_org && !regexp("$(badhostsre)", Machine)
rank = KFlops

executable = $scratchdir/$exefile
arguments = \$(inputfile) $desturl/\$(inputfile)/$timestamp/\$(njob)

transfer_input_files = $scratchdir/$pkgfile, $inputfiledir/\$(inputfile)
+TransferOutput = ""

output = $scratchdir/stdouterr/\$(inputfile)/\$(njob).out
error = $scratchdir/stdouterr/\$(inputfile)/\$(njob).err

periodic_release = (HoldReasonCode =?= 12) || (HoldReasonCode =?= 13)
periodic_remove = (HoldReasonCode =?= 1) && (NumJobStarts >= 6)

queue
EOF

# create a dag template with entries for each input file
# and replacement strings for the job numbers
njob_replace='{NJOB}'

for f in $inputfiles; do
  inputfile=$(basename $f)

  # create directories for job stdout and stderr files
  mkdir -p $scratchdir/stdouterr/$inputfile

  # write dag lines
  job="${inputfile}/${njob_replace}"
  cat >> $scratchdir/dag.template <<EOF
JOB $job $scratchdir/job
VARS $job inputfile="$inputfile" njob="$njob_replace"
RETRY $job 3
EOF
done

# create the actual condor dag file from the template
echo 'writing dag file...'
jobsperinput=$(( jobsperinput - 1 ))
for njob in $(seq -w 0 $jobsperinput); do
  # progress indicator
  # (it can take a moment to generate the full dag file)
  echo -ne "\r$njob / $jobsperinput"
  sed "s/${njob_replace}/${njob}/g" $scratchdir/dag.template >> $scratchdir/dag
done
echo -e "\ndone"

# go!
condor_submit_dag -maxidle 1000 $scratchdir/dag
