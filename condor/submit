#!/bin/bash

if ! voms-proxy-info -exists -hours 10 2> /dev/null; then
  echo 'create a proxy first'
  exit 1
fi

if [[ $# < 3 ]]; then
  echo "usage: $0 batch_label jobs_per_input_file input_files..."
  exit 1
fi

# parse arguments
batchlabel=$1
jobsperinput=$2
inputfiles=${@:3}

# every batch needs a unique ID -- a timestamp works well
timestamp="$(date +%Y%m%d_%H%M%S)"

# gridftp destination for all events in this batch
desturl="gsiftp://ntheoryfs01.phy.duke.edu/var/phy/project/nukeserv/$USER/hic-events/$batchlabel"

# directory containing this script
# http://stackoverflow.com/a/246128
startdir=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)

# create a "scratch" directory for all the condor files
scratchdir="/local-scratch/$USER/condor/$batchlabel/$timestamp"
mkdir -p $scratchdir

# Create a condor submit description file.  This is the "template" job file for
# every job in this batch with several $(...) variables to be set by the dag.
cat > $scratchdir/job <<EOF
universe = vanilla
x509userproxy = /tmp/x509up_u$(id --user)
+ProjectName = "Duke-QGP"

request_memory = 500M
request_disk = 1G
requirements = HAS_CVMFS_oasis_opensciencegrid_org
rank = KFlops

executable = $startdir/hic-wrapper
arguments = \$(inputfile) $desturl/\$(inputfile)/$timestamp/\$(njob)

transfer_input_files = $startdir/../hic-osg.tar.gz, \$(inputfilepath)
+TransferOutput = ""

output = $scratchdir/stdouterr/\$(inputfile)/\$(njob).out
error = $scratchdir/stdouterr/\$(inputfile)/\$(njob).err

periodic_release = (HoldReasonCode =?= 13)
periodic_remove = (HoldReasonCode =?= 1) && (NumJobStarts >= 6)

queue
EOF

# create a dag template with entries for each input file
# and replacement strings for the job numbers
njob_replace='{NJOB}'

for f in $inputfiles; do
  inputfile=$(basename $f)
  inputfilepath=$(readlink -e $f)

  # create directories for job stdout and stderr files
  mkdir -p $scratchdir/stdouterr/$inputfile

  # write dag lines
  job="${inputfile}_${njob_replace}"
  cat >> $scratchdir/dag.template <<EOF
JOB $job $scratchdir/job
VARS $job inputfile="$inputfile" inputfilepath="$inputfilepath" njob="$njob_replace"
RETRY $job 3
EOF
done

# create the actual condor dag file from the template
for njob in $(seq -w 0 $(( jobsperinput - 1 ))); do
  # progress indicator
  # (it can take a moment to generate the full dag file)
  echo $njob
  sed "s/${njob_replace}/${njob}/g" $scratchdir/dag.template >> $scratchdir/dag
done

# go!
condor_submit_dag -maxidle 10000 $scratchdir/dag
