#!/usr/bin/env python3

"""
Runs several complete heavy-ion collision events:

  - initial condition (trento)
  - free streaming
  - hydro (vishnew)
  - particlization / sampler (frzout)
  - hadronic afterburner (urqmd)

If an argument is passed, it is read as a configuration file with simple
key = value syntax and stored in the 'config' dict.  Values are then accessed
with 'config.get()' and hence all have reasonable defaults.

For each event, computes observables (multiplicities, mean transverse momenta,
flow vectors, etc) and saves them in raw binary file 'results', which may be
read by np.fromfile() using the same dtype as the 'results' array.

Some advantages of this format:

  - speed and file size (zero overhead)
  - files may be concatened together
  - easy I/O in numpy (fromfile/tofile)

A disadvantage:

  - no metadata is saved, so the precise data type must be known and fully
    specified or the files are nonsense

"""

from itertools import chain
import sys
import subprocess

import numpy as np
import h5py

# load libraries distributed with hic-osg
sys.path.insert(1, 'lib/python')
import freestream
import frzout


def run_cmd(*args, **kwargs):
    """
    Run a subprocess, concatenating argument strings together.

    """
    print(*args, flush=True)  # flush stdout to retain output order
    subprocess.check_call(
        list(chain.from_iterable(a.split() for a in args)),
        **kwargs
    )


def read_text_file(filename):
    """
    Read a text file into a nested list of bytes objects,
    skipping comment lines (#).

    """
    with open(filename, 'rb') as f:
        return [l.split() for l in f if not l.startswith(b'#')]


def main():
    # parse config file
    if len(sys.argv) == 2:
        with open(sys.argv[1], 'r') as f:
            config = dict(
                (i.strip() for i in l.split('=', maxsplit=1))
                for l in f
            )
    else:
        config = {}

    nevents = 10
    grid_step = 0.1
    grid_max = 13.05

    # run trento and yield initial entropy density arrays
    def initial_conditions(initial_file='initial.hdf'):
        run_cmd(
            './bin/trento Pb Pb', str(nevents),
            '--cross-section 6.4',
            '--grid-step {} --grid-max {}'.format(grid_step, grid_max),
            '--output', initial_file,
            config.get('trento_args', '')
        )

        with h5py.File(initial_file, 'r') as f:
            for dset in f.values():
                yield np.array(dset)

    # read free streaming time and enable for time > epsilon
    tau_fs = float(config.get('tau_fs', 0))
    enable_fs = tau_fs > 1e-6

    # create sampler HRG object (to be reused for all events)
    Tswitch = float(config.get('Tswitch', .15))
    hrg = frzout.HRG(Tswitch, species='urqmd', res_width=True)
    eswitch = hrg.energy_density()

    # append switching energy density to vishnew arguments
    vishnew_args = [
        config.get('vishnew_args', ''),
        'initialuread=1 iein=0 t0={}'.format(tau_fs) if enable_fs else
        'initialuread=0 iein=1',
        'edec={}'.format(eswitch)
    ]

    # species (name, ID) for identified particle observables
    species = [
        ('pion', 211),
        ('kaon', 321),
        ('proton', 2212),
    ]

    # fully specify numeric data types, including endianness and size, to
    # ensure consistency across all machines
    float_t = '<f8'
    int_t = '<i8'
    complex_t = '<c16'

    # allocate results array (see docstring at top of file)
    results = np.zeros(
        nevents, dtype=[
            ('initial_entropy', float_t),
            ('mult_factor', float_t),
            ('dNch_deta', float_t),
            ('dN_dy', [(s, float_t) for (s, _) in species]),
            ('mean_pT', [(s, float_t) for (s, _) in species]),
            ('M', int_t),
            ('Qn', complex_t, 6),
        ]
    )

    # run each event
    for ic, res in zip(initial_conditions(), results):
        res['initial_entropy'] = grid_step**2 * ic.sum()

        # write initial file[s] for vishnew
        if enable_fs:
            # free stream initial condition
            fs = freestream.FreeStreamer(ic, grid_max, tau_fs)

            e = fs.energy_density()
            e_above = e[e > eswitch].sum()
            res['mult_factor'] = e.sum()/e_above if e_above > 0 else 1

            np.savetxt('vishnew/ed.dat', e)
            for i in [1, 2]:
                np.savetxt('vishnew/u{}.dat'.format(i), fs.flow_velocity(i))
            for ij in [(1, 1), (1, 2), (2, 2)]:
                np.savetxt(
                    'vishnew/pi{}{}.dat'.format(*ij), fs.shear_tensor(*ij))
        else:
            # skip free streaming, use initial condition as entropy density
            np.savetxt('vishnew/sd.dat', ic)

        # hydro
        run_cmd('./vishnew', *vishnew_args, cwd='vishnew')

        # read freeze-out surface data
        surface_data = np.array(
            read_text_file('vishnew/surface.dat'),
            dtype=float
        )

        # end event if the surface is empty -- this occurs in ultra-peripheral
        # events where the initial condition doesn't exceed Tswitch
        if surface_data.size == 0:
            print('empty hypersurface')
            continue

        # unpack surface_data columns:
        #   0    1  2  3         4         5         6    7
        #   tau  x  y  dsigma^t  dsigma^x  dsigma^y  v_x  v_y
        #   8     9     10    11    12    13    14    15
        #   pitt  pitx  pity  pixx  pixy  piyy  pizz  Pi
        x, sigma, v, _ = np.hsplit(surface_data, [3, 6, 8])
        pi = dict(zip(['xx', 'xy', 'yy'], surface_data.T[11:14]))
        Pi = surface_data.T[15]

        # create sampler surface object
        surface = frzout.Surface(x, sigma, v, pi=pi, Pi=Pi, ymax=2)

        minsamples, maxsamples = 10, 1000  # reasonable range for nsamples
        minparts = 10**5  # min number of particles to sample
        nparts = 0  # for tracking total number of sampled particles

        # sample particles and write to file
        with open('urqmd-afterburner/initial.dat', 'w') as f:
            for nsamples in range(1, maxsamples + 1):
                parts = frzout.sample(surface, hrg)
                if parts.size == 0:
                    continue
                nparts += parts.size
                print('#', parts.size, file=f)
                for p in parts:
                    print(p['ID'], *chain(p['x'], p['p']), file=f)
                if nparts >= minparts and nsamples >= minsamples:
                    break

        print('nsamples =', nsamples)
        print('nparts =', nparts)

        if nparts == 0:
            continue

        # hadronic afterburner
        run_cmd('./afterburner initial.dat final.dat', cwd='urqmd-afterburner')

        # read final particle data
        ID, charge, pT, phi, y, eta = (
            np.array(col, dtype=dtype) for (col, dtype) in
            zip(
                zip(*read_text_file('urqmd-afterburner/final.dat')),
                (2*[int] + 4*[float])
            )
        )

        # compute observables
        charged = (charge != 0)
        abs_eta = np.fabs(eta)

        res['dNch_deta'] = \
            np.count_nonzero(charged & (abs_eta < .5)) / nsamples

        abs_ID = np.abs(ID)
        midrapidity = (np.fabs(y) < .5)

        for name, i in species:
            cut = (abs_ID == i) & midrapidity
            N = np.count_nonzero(cut)
            res['dN_dy'][name] = N / nsamples
            res['mean_pT'][name] = (0. if N == 0 else pT[cut].mean())

        phi_alice = phi[charged & (abs_eta < .8) & (.2 < pT) & (pT < 5.)]
        res['M'] = phi_alice.size
        res['Qn'] = [np.exp(1j*n*phi_alice).sum() for n in range(1, 7)]

    results.tofile('results')


if __name__ == "__main__":
    main()
